# Welcome to Serverless!
#
# This file is the main config file for your service.
# It's very minimal at this point and uses default values.
# You can always add more config options for more control.
# We've included some commented out config examples here.
# Just uncomment any of them to get that config option.
#
# For full config options, check the docs:
#    docs.serverless.com
#
# Happy Coding!

service: ocsp

# You can pin your service to only deploy with a specific Serverless version
# Check out our docs for more details
# frameworkVersion: "=X.X.X"
#frameworkVersion: ">=1.1.0 <2.0.0"

provider:
  name: aws
  runtime: python3.6
  region: ap-southeast-1
  stage: dev
  memorySize: 128
  environment:
    DYNAMODB_REGION: ${opt:region, self:provider.region}
    DYNAMODB_TABLE: ${self:service}-${opt:stage, self:provider.stage}
    DYNAMODB_DATETIME_FORMAT: '%Y-%m-%dT%H:%M:%S.%f%z'
  iamRoleStatements:
    - Effect: Allow
      Action:
        - dynamodb:Query
        - dynamodb:Scan
        - dynamodb:GetItem
        - dynamodb:PutItem
        - dynamodb:UpdateItem
        - dynamodb:DeleteItem
        - dynamodb:DescribeTable
      Resource: "arn:aws:dynamodb:${opt:region, self:provider.region}:*:table/${self:provider.environment.DYNAMODB_TABLE}"
    - Effect: "Allow"
      Action:
        - "s3:GetObject"
      Resource: "arn:aws:s3:::${self:custom.s3_bucket}/${self:custom.s3_key_base}*"
    - Effect: "Allow"
      Action:
        - "s3:PutBucketNotification"
      Resource: "arn:aws:s3:::${self:custom.s3_bucket}"

# you can overwrite defaults here
#  stage: dev
#  region: us-east-1

# you can add statements to the Lambda function's IAM Role here
#  iamRoleStatements:
#    - Effect: "Allow"
#      Action:
#        - "s3:ListBucket"
#      Resource: { "Fn::Join" : ["", ["arn:aws:s3:::", { "Ref" : "ServerlessDeploymentBucket" } ] ]  }
#    - Effect: "Allow"
#      Action:
#        - "s3:PutObject"
#      Resource:
#        Fn::Join:
#          - ""
#          - - "arn:aws:s3:::"
#            - "Ref" : "ServerlessDeploymentBucket"
#            - "/*"

# you can define service wide environment variables here
#  environment:
#    variable1: value1

# you can add packaging information here
#package:
#  include:
#    - include-me.py
#    - include-me-dir/**
#  exclude:
#    - exclude-me.py
#    - exclude-me-dir/**

package:
  excludeDevDependencies: true
  exclude:
    - .gitignore
    - .git/**
    - node_modules/**
    - .idea/**
    - .requirements/**
    - env/**
    - README.md
    - package.json
    - package-lock.json
    - requirements.txt
    - Pipfile
    - Pipfile.lock
    - '**.dist-info/**'
    - LICENSE


functions:
  respond:
    description: Online Certificate Status Protocol via HTTP GET / POST
    handler: handler.respond
    timeout: 15
    environment:
      # OCSP_COUNT should denote the total number of OCSP responders (in-case of serving intermediate-CAs)
      OCSP_COUNT: '2'
      # PFX files should contain ocsp signer's privKey (and all related certificates in the chain)
      OCSP_PFX_1: 'config/ocsp1.pfx'
      OCSP_PFX_PASS_1: 'rahasia123456'
      # 2nd OCSP Responder/Signer
      OCSP_PFX_2: 'config/ocsp2.pfx'
      OCSP_PFX_PASS_2: '123456'
    events:
      - http:
          path: /{request_b64}
          method: get
          cors: true
          contentHandling: CONVERT_TO_BINARY
      - http:
          path: /
          method: post
          cors: true
          contentHandling: CONVERT_TO_BINARY
  bucket:
    handler: bucket.event
    description: Called by s3 create/remove events to manage certificate state in DynamoDB
    timeout: 60
    environment:
      # One or many Root certificates in PEM-format to validate the certificates before being registered into OCSP
      TRUST_ROOTS: 'config/root_bundle.pem'
    events:
      - existingS3:
          bucket: ${self:custom.s3_bucket}
          event:
            - s3:ObjectCreated:*
            - s3:ObjectRemoved:*
          rules:
            - prefix: ${self:custom.s3_key_base}
      # - s3:
      #     bucket: ${self:custom.s3_bucket}
      #     event: s3:ObjectCreated:*
      #     rules:
      #       - prefix: ${self:custom.s3_key_base}
      # - s3:
      #     bucket: ${self:custom.s3_bucket}
      #     event: s3:ObjectRemoved:*
      #     rules:
      #       - prefix: ${self:custom.s3_key_base}

#    The following are a few example events you can configure
#    NOTE: Please make sure to change your handler code to work with those events
#    Check the event documentation for details
#    events:
#      - http:
#          path: users/create
#          method: get
#      - s3: ${env:BUCKET}
#      - schedule: rate(10 minutes)
#      - sns: greeter-topic
#      - stream: arn:aws:dynamodb:region:XXXXXX:table/foo/stream/1970-01-01T00:00:00.000
#      - alexaSkill
#      - alexaSmartHome: amzn1.ask.skill.xx-xx-xx-xx
#      - iot:
#          sql: "SELECT * FROM 'some_topic'"
#      - cloudwatchEvent:
#          event:
#            source:
#              - "aws.ec2"
#            detail-type:
#              - "EC2 Instance State-change Notification"
#            detail:
#              state:
#                - pending
#      - cloudwatchLog: '/aws/lambda/hello'
#      - cognitoUserPool:
#          pool: MyUserPool
#          trigger: PreSignUp

#    Define function environment variables here
#    environment:
#      variable2: value2

# you can add CloudFormation resource templates here
#resources:
#  Resources:
#    NewResource:
#      Type: AWS::S3::Bucket
#      Properties:
#        BucketName: my-new-bucket
#  Outputs:
#     NewOutput:
#       Description: "Description for the output"
#       Value: "Some output value"

resources:
  Resources:
    OcspDynamoDbTable:
      Type: 'AWS::DynamoDB::Table'
      #DeletionPolicy: Retain
      Properties:
        AttributeDefinitions:
          -
            AttributeName: uid
            AttributeType: S
        KeySchema:
          -
            AttributeName: uid
            KeyType: HASH
        ProvisionedThroughput:
          ReadCapacityUnits: 1
          WriteCapacityUnits: 1
        TableName: ${self:provider.environment.DYNAMODB_TABLE}

custom:
  pythonRequirements:
    usePipenv: true
    #dockerizePip: true
    #dockerImage: heri16/lambda:build-python3.6
    #zip: true
  customDomain:
    domainName: pki.lmu.co.id
    basePath: ocsp
    createRoute53Record: true
    endpointType: 'regional'
  apigatewayBinary:
    types:
      - 'application/ocsp-request'  # request content-type-header
      - '*/*'                       # request accept-header
      #- 'application/json'
      #- 'image/jpeg'
      # list of mime-types (matches request content-type-header for request-body, and matches request accept-header for response-body)
      # Read carefully: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings.html
      # https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-payload-encodings-workflow.html
  s3_bucket: lmu-pki
  s3_key_base: certs/

plugins:
  - serverless-python-requirements
  - serverless-domain-manager
  - serverless-plugin-custom-binary
  - serverless-plugin-existing-s3
  # https://medium.com/nextfaze/binary-responses-with-serverless-framework-and-api-gateway-5fde91376b76
  #- serverless-apigateway-plugin 
  #- serverless-apigwy-binary
