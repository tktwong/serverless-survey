
service: kinesis-lambda-sqs-tutorial
# app and org for use with dashboard.serverless.com
app: kinesis-lambda-sqs-tutorial
org: jwan622

frameworkVersion: "=1.60.5"

provider:
  name: aws
  runtime: python3.8
  memorySize: 128
  timeout: 4
  # this is the s3 bucket that will store the lambda zip file that composes of the lambda code and dependencies
  deploymentBucket: kinesis-lambda-sqs-tutorial-s3-bucket
  region: us-east-1
  stage: dev

# you can add statements to the Lambda function's IAM Role here
#  iamRoleStatements:
#    - Effect: "Allow"
#      Action:
#        - "s3:ListBucket"
#      Resource: { "Fn::Join" : ["", ["arn:aws:s3:::", { "Ref" : "ServerlessDeploymentBucket" } ] ]  }
#    - Effect: "Allow"
#      Action:
#        - "s3:PutObject"
#      Resource:
#        Fn::Join:
#          - ""
#          - - "arn:aws:s3:::"
#            - "Ref" : "ServerlessDeploymentBucket"
#            - "/*"

functions:
  kinesisStreamReader:
    # readEvents is the function name that will be called
    handler: handler.readEvents
    description: "Stride lambda deployed using serverless framework. The description was set in a serverless yaml file."
    # the lambda can have environment variables
    environment:
      example_env_var: some_env_var
    # this is how to specify the kinesis event trigger and configure stream.
    events:
      - stream:
          type: kinesis
          arn: arn:aws:kinesis:us-east-1:516088479088:stream/kinesis-lambda-sqs-tutorial-stream
          # In the request, you can specify the shard iterator type AT_TIMESTAMP to read records from an arbitrary point in time,
          # TRIM_HORIZON to cause ShardIterator to point to the last untrimmed record in the shard in the system (the oldest data record in the shard), or
          # LATEST so that you always read the most recent data in the shard.
          startingPosition: TRIM_HORIZON
          batchSize: 10
          enabled: false
#    The following are a few example events you can configure
#    NOTE: Please make sure to change your handler code to work with those events
#    Check the event documentation for details
#    events:
#      - http:
#          path: users/create
#          method: get
#      - websocket: $connect
#      - s3: ${env:BUCKET}
#      - schedule: rate(10 minutes)
#      - sns: greeter-topic
#      - stream: arn:aws:dynamodb:region:XXXXXX:table/foo/stream/1970-01-01T00:00:00.000
#      - alexaSkill: amzn1.ask.skill.xx-xx-xx-xx
#      - alexaSmartHome: amzn1.ask.skill.xx-xx-xx-xx
#      - iot:
#          sql: "SELECT * FROM 'some_topic'"
#      - cloudwatchEvent:
#          event:
#            source:
#              - "aws.ec2"
#            detail-type:
#              - "EC2 Instance State-change Notification"
#            detail:
#              state:
#                -  pending
#      - cloudwatchLog: '/aws/lambda/hello'
#      - cognitoUserPool:
#          pool: MyUserPool
#          trigger: PreSignUp
#      - alb:
#          listenerArn: arn:aws:elasticloadbalancing:us-east-1:XXXXXX:listener/app/my-load-balancer/50dc6c495c0c9188/
#          priority: 1
#          conditions:
#            host: example.com
#            path: /hello

#    Define function environment variables here
#    environment:
#      variable2: value2

# you can add CloudFormation resource templates here
#resources:
#  Resources:
#    NewResource:
#      Type: AWS::S3::Bucket
#      Properties:
#        BucketName: my-new-bucket
#  Outputs:
#     NewOutput:
#       Description: "Description for the output"
#       Value: "Some output value"
