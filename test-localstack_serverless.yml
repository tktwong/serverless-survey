
service: test-localstack

provider:
  name: aws
  runtime: nodejs12.x
  region: us-east-1
  stage: dev
  profile: default
  environment:
    aws_region: ${self:provider.region}
    es_host: localhost
    es_port: 9200
    es_input_index: input
    es_output_index: output
    KINESIS_STREAM: in_stream
    # s3_bucket: exports
    # s3_endpoint: http://localhost:8000

plugins:
#  - serverless-s3-local
  - serverless-offline-kinesis
  - serverless-offline

package:
  exclude:
    - .elasticsearch
    - .localstack
    - node_modules
    - docker-compose.yml
    - notes.md
    - .gitignore
    - tmps3/**

custom:
  serverless-offline:
    httpPort: 4000
  serverless-offline-kinesis:
    endpoint: http://localhost:4567
    accessKeyId: local
    secretAccessKey: local
  # s3:
  #   host: localhost
  #   port: 8000
  #   directory: tmps3

functions:
  producer:
    handler: handler.producer
    timeout: 5
    events:
      - http:
          path: produce
          method: GET
  consumer:
    handler: handler.consumer
    timeout: 1
    events:
      - stream:
          enabled: true
          type: kinesis
          batchSize: 10
          startingPosition: LATEST
          arn: arn:aws:kinesis:region:XXXXXX:stream/in_stream
  tester:
    handler: handler.tester
    events:
      - http:
          path: test
          method: GET
  # export:
  #   handler: handler.cronjob
  #   timeout: 600
  #   events:
  #     - schedule:
  #         rate: rate(1 minute)
  #         name: cron_export_csv
  #         description: 'Exports articles weekly into S3 bucket in csv format'
  #         enabled: true
  #         input:
  #           format: csv

resources:
   Resources:
#    NewResource:
#      Type: AWS::S3::Bucket
#      Properties:
#        BucketName: ${self.provider.environment.s3_bucket}
    INPUT_STREAM:
      Type: AWS::Kinesis::Stream
      Properties:
        Name: ${self:provider.environment.KINESIS_STREAM}
        ShardCount: 1
